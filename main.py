"""
Mushroom Classification Script using a Naive Bayes Classifier.

This script demonstrates a full end-to-end workflow for a simple Naive Bayes
classifier built from scratch. It performs the following steps:

1.  Loads and pre-processes the mushroom dataset.
2.  Splits the data into training and testing sets, ensuring the test set
    contains only values seen during training.
3.  "Trains" a Naive Bayes model by counting frequencies from the training data.
4.  Converts these counts into log-probability weights.
5.  Interactively prompts the user to input features for a new mushroom sample.
6.  Classifies the user-provided sample and prints the prediction.

To run the script, execute from the command line:
    python your_script_name.py
"""

from copy import deepcopy
import pandas as pd
from math import log
from sklearn.model_selection import train_test_split
from typing import Tuple, Any

from nb_classifier import DataHandler


def get_model(data: pd.DataFrame, target_col: str) -> dict:
    """
    Builds a "model" for a Naive Bayes classifier by counting occurrences.

    This function iterates through the dataset to compute counts for prior
    probabilities (target counts) and likelihoods (feature counts conditioned
    on the target). It is designed for categorical data.

    Args:
        data (pd.DataFrame): The DataFrame containing the training data.
        target_col (str): The name of the target variable column.

    Returns:
        dict: A dictionary containing the trained model's components:
              - 'likelihoods': Nested dictionary of feature counts per target class.
              - 'target_counts': Dictionary of counts for each target class.
              - 'total_count': Total number of rows in the data.
    """
    # Initialize a dictionary to store the counts of each target value, starting at 0.
    target_counts = {val: 0 for val in data[target_col].unique()}

    # Create a template dictionary for feature counts, to be used for each target value.
    # This ensures all possible feature values are included for every target class.
    unique_value_dicts = {
        col: {val: 0 for val in data[col].unique()}
        for col in data.columns
        if col != target_col
    }

    # Initialize the main likelihoods dictionary.
    likelihoods = dict()
    # Populate the likelihoods dictionary, giving each target class its own feature count template.
    for val in data[target_col].unique():
        likelihoods[val] = deepcopy(unique_value_dicts)

    # Iterate over each row to count occurrences of each feature/target combination.
    for index, row in data.iterrows():
        target_value = row[target_col]
        # Increment the count for the current row's target value.
        target_counts[target_value] += 1
        # For each feature, increment the count for the specific feature value given the target value.
        for col in data.columns:
            if col != target_col:
                likelihoods[target_value][col][row[col]] += 1

    # Assemble the final result dictionary.
    result = {
        "likelihoods": likelihoods,
        "target_counts": target_counts,
        "total_count": sum(target_counts.values()),
    }

    return result


def convert_counts_to_weights(model: dict, alpha: float = 1.0) -> dict:
    """
    Converts a model of raw counts into a model of log-probabilities (weights).

    This function takes the count-based model generated by `get_model` and
    transforms it into log-probabilities suitable for a Naive Bayes classifier.
    It calculates:
    1. The log of the prior probability for each target class.
    2. The log of the likelihood for each feature value, conditioned on the target class.

    It applies Lidstone/Laplace smoothing (`alpha`) conditionally: only if a feature
    for a given class contains a zero count for any of its values.

    Args:
        model (dict): A dictionary containing 'likelihoods', 'target_counts',
                      and 'total_count' from the `get_model` function.
        alpha (float, optional): The smoothing parameter (additive factor).
                                 Defaults to 1.0 (Laplace smoothing).

    Returns:
        dict: A new dictionary of weights where counts are replaced by
              log-probabilities.
    """
    likelihoods = model["likelihoods"]
    target_counts = model["target_counts"]
    total_count = model["total_count"]
    weights = {}

    # Use deepcopy to avoid modifying the original model dictionary
    for target_value, column_dict in deepcopy(likelihoods).items():
        weights[target_value] = {}

        # Calculate the log prior probability for the current target class
        prior = log(target_counts[target_value] / total_count)
        weights[target_value]["__prior__"] = prior

        # Iterate through each feature for the current target class
        for column, value_dict in column_dict.items():
            # Conditionally apply smoothing: only if a zero count is detected
            if any(v == 0 for v in value_dict.values()):
                value_dict = {k: v + alpha for k, v in value_dict.items()}

            # Calculate the total count for this feature, post-smoothing if applied
            total = sum(value_dict.values())

            # Calculate the log likelihood for each possible value of the feature
            weights[target_value][column] = {
                k: log(v / total) for k, v in value_dict.items()
            }

    return weights


def get_user_sample(data: pd.DataFrame, target_col: str) -> dict:
    """
    Interactively prompts the user to build a sample data point.

    This function iterates through each feature column of the provided DataFrame
    (excluding the target column), displays the unique values as numbered options,
    and asks the user to select one. The selections are compiled into a
    dictionary representing a single data sample.

    Args:
        data (pd.DataFrame): The DataFrame from which to source the feature columns
                             and their possible values.
        target_col (str): The name of the target column, which will be ignored.

    Returns:
        dict: A dictionary where keys are feature column names and values are
              the choices made by the user.
    """
    sample = {}

    for col in data.columns:
        if col == target_col:
            continue

        options = list(data[col].unique())

        print(f"\nSelect a value for the column: {col}")
        for i, opt in enumerate(options):
            print(f"  {i + 1}. {opt}")

        choice = int(input("Enter number: ")) - 1
        sample[col] = options[choice]

    return sample


def classify_sample_strict(sample: dict, weights: dict) -> Any:
    """
    Classifies a sample using pre-calculated log-probability weights with strict validation.

    This function implements a Naive Bayes classifier by calculating the log-probability
    for each possible class and returning the class with the highest score.

    This version is "strict": it will raise a `ValueError` if it encounters a
    feature-value pair in the sample that was not seen during training.

    Args:
        sample (dict): A dictionary representing the data sample to classify.
        weights (dict): A dictionary of pre-calculated log-probabilities.

    Returns:
        Any: The predicted class label with the highest calculated log-probability.

    Raises:
        ValueError: If a value in the `sample` does not exist in the weights.
    """
    best_class = None
    best_log_prob = float("-inf")

    for target_value in weights.keys():
        log_prob = weights[target_value]["__prior__"]

        for feature, value in sample.items():
            if value not in weights[target_value].get(feature, {}):
                raise ValueError(f"Unseen value encountered: {feature} = {value}")
            log_prob += weights[target_value][feature][value]

        if log_prob > best_log_prob:
            best_log_prob = log_prob
            best_class = target_value

    return best_class


if __name__ == "__main__":

    # --- 1. Data Loading and Pre-processing ---
    data_manager = DataHandler(data_path='data/mushroom_decoded.csv')
    mushroom_data = data_manager.load_and_clean()
    train_data, test_data = data_manager.split_and_validate(
        mushroom_data, target_col="poisonous"
    )

    # --- 2. Model Training ---
    # Build the count-based model from the training data.
    mushroom_model = get_model(train_data, "poisonous")

    # Convert the raw counts into log-probability weights for classification.
    mushroom_weights = convert_counts_to_weights(mushroom_model)

    # --- 3. Interactive Classification ---
    # Prompt the user to create a new sample.
    sample = get_user_sample(mushroom_data, target_col="poisonous")

    # Classify the user's sample using the trained model.
    prediction = classify_sample_strict(sample, mushroom_weights)

    # Display the results to the user.
    print(f"\nYour sample: {sample}")
    print(f"The model predicts the mushroom is: {prediction}")
